---
title: "Exploring the Strategic Business Impact of Common Issues Complicating Value Delivered by Software Test Engineering"
publishDate: "2024-06-11T17:51:29-05:00"
coverPhoto: "https://static.trevorwagner.dev/images/kai-pilger-1k3vsv7iIIc-unsplash/1200x800.jpg"
thumbnail: "https://static.trevorwagner.dev/images/kai-pilger-1k3vsv7iIIc-unsplash/300x200.jpg"
draft: false

---

Like with nearly any other form of engineering, the value Software Test Engineering delivers is derived from the solutions it produces working as expected. When solutions developed within Test Engineering deliver the expected value, the software development organizations they are developed for ultimately gain some sort of access to a well-functioning, performant, easy-to-troubleshoot, and extensible feedback loop that provides reliable insight into what the functional state of work product is now and how that functional state might be affected in response to a change of some sort. In essence, usable testing shines a light on the things that might exhibit risk: by making functional state easier to evaluate (by, at the very least, making it visible), testing also makes risk easier (also commonly _more efficient_) for the organization to define and ultimately accommodate in planning.

In essence, then: well-functioning testing efforts serve to provide data used to inform (at however arguably low-volume, and whatever the type of organization) **business intelligence** related to what impact current functional state may be expected to have, either on functionality itself or on business interests related to it. And at the same time development itself is of strategic benefit to a software development organization, so is testing that is sufficient, reliable, and performant.

Frequently, though, software development organizations encounter issues where solutions produced by Test Engineering do not sufficiently deliver the value they are expected to. At best, this limits the strategic benefit the organization can expect to enjoy from the sort of business intelligence derived from testing. In addition to this, though, the weight of issues like these tends to extend beyond the radius immediate to testing, to a point where they also present drag to or potentially risk capsizing other (often strategic) efforts within the organization.

What this post will explore (at a very high level, and irrespective of concerns related to cost and budget) is what the strategic impact of these issues to software development generally looks like. With hope, exploring what doesn't work can provide insight into what does (or should) work in Test Engineering nominally, much the same way it often does with functionality currently in flight.

## Obstructions to Strategic Initiatives
In general, software development is an effort undertaken with the intent of reaching a strategic objective of projected net benefit. Whether functionality is developed to support a specific set of use cases, to make a specific set of operations more efficient, or to target- or expand within a specific market (or market segment), the ambition that drives software development is the same that drives any investment: _to create and capitalize on_ (to whatever degree) _opportunities where the projected benefits of success outweigh the projected risks_. And when considering dependency relationships, the relationship between risk and benefit is not always one-to-one. Sometimes success in one area can unlock development in multiple other strategic areas (perhaps worth noting that this is as true with Test Engineering as it is with general Software Engineering). Sometimes, then, the relationship between risk and reward can be very complex even it's clear where the risk lies.

As noted in an often-used quote attributed to Benjamin Franklin: _an ounce of prevention is worth a pound of cure_. And software development organizations don't simply choose to develop or release (or not, in either case) based on this relationship (between risk and benefit); they use assessments of this relationship to inform prioritizing development based on _informed consent_ to risk as well as to _mitigate or neutralize sources of outstanding risk_ it has made itself aware of.

If an organization is not able to effectively evaluate current functional state, it also won't be able to assess risk for a system under test. This includes:

- Lack of sufficient tooling to be able to test- or cover the system.
- Lack of existing coverage around the system.
- Lack of confidence in existing Test Engineering resources to be able to plan-, implement-, or execute testing or coverage.
- Lack of a sufficient, efficient, or sound testing strategy (the approach to informing how to exercise the system under test and gather test results or other outputs for evaluation).

Rather than risk costly surprises that may follow a release with insufficient evidence, leadership will frequently deprioritize high-risk development (however tantalizing the projected strategic benefits) if they don't have the capability or the bandwidth to get sufficient insight into current functional state or risk associated with it. Ultimately it depends on the nature of the risk and the availability/ lack of information. But if leadership is risk-averse (or generally intolerant of a mess), lack of insight ultimately means that any consent to risk (or planning in response to it) would effectively be uninformed.

## Challenges to the Effectiveness of Testing Efforts
As mentioned (somewhat) in the introduction, one of the primary outputs of the feedback loop that testing contributes to is **insight into the current functional state of work product**; again, this serves as data to inform business intelligence. As with any intelligence, the intelligence informed by testing should be timely and actionable; the latter also supposes clear signal for noise. In essence, tests (manual or automated) should run smoothly and efficiently. The cost of implementation should be minimal, as should any overhead anticipated from the eventual need to troubleshoot, extend, refactor, or migrate testing capability if needed.

Issues challenging these benefits don't just challenge the ability to inform business intelligence; they also present operational challenges on their own that consume resources either to resolve or circumnavigate. As examples, consider:

- Automated test suites needing to be re-executed in CI in order to produce a run with a passing result/ collective passing result between runs (or which fail intermittently halfway through unreasonably long runtime).
- Testing or coverage available (or implemented) exclusively at the end-to-end level, despite the fact that the system unnder test is open to testing at a lower level or within a different configuration.
- Manual tests with steps described and stored with full verbosity in a manner that presents challenges to organize or differentiate.
- Frameworks that are neither extensible nor open to refactoring, or which make troubleshooting a challenge.
- Automated tests that are designed or implmented in such a way that they fail to catch the expected issues (or potentially mask other issues).

The longer any of these issues is allowed to continue unresolved, the longer it accumulates mass until (typically) it collapses into what effectively serves as an operations black hole.

Eventually, the gravity associated with issues like these becomes sufficient that it creates drag elsewhere within the organization. For example:

- Issues with tests- or test support may hamper or even prevent the implementation or execution of additional testing or coverage (or even functionality -- see the previous section, above).
- Testing may fail to make the relationship between inputs (efforts to define- and excute tests) and outputs (the data returned by tests) clear and easy to evaluate or even plan for.
- Tests can delay or block delivery of value through development: for example, how many times will a team need to re-run flaky automated tests in CI in order to be able to power through merging completed changes?
- If automated tests need to be re-run frequently in CI, it may have impact on costs for services related to cloud hosting or CI services supporting test automation.
- Long testing tails for manual release testing hold the organziation back from being able to relesae (thereby capitalize on) completed development work.

At some point testing (generally or in context of any of these issues) starts to appear sunk-cost: not only will it take resources to resolve any of these issues, but any bandwidth spent attmepting to mitigate- or circumnavigate any of these issues as an alternative may ultimately prove (upon resolution) to have been spent in vain. Nobody wants to throw good money after bad, but oftentimes if an organization does not in these cases, it may lose access to testing or coverage (which, again, is a loss of strategic benefit from Test Engineering).

At an extreme, this might even raise questions related to why an organization dedicates resources to testing (or even if they should test or innovate in testing) at all. If return on investment is potentially net negative, why invest?

## Unwelcome Trade-Offs
In order to produce solutions in Test Engineering, software development organizations need to source bandwidth (talent, cycle time, etc.) from somewhere. There may- or may not be available bandwidth within the organization. And for anybody who has tried hiring (or even interviewing) for Test Engineering is likely aware, there may- or may not be suitable talent (or even adaptable talent) available within the candidate pool. Either way, the work still needs to get done; no reason to make perfect the enemy of the good. In the face of any of this, software development organizations will frequently source bandwidth from elsewhere within the organization or by way of an off-the-shelf solution as a temporary (or in some cases mid- or even long-term) stopgap for lack of available bandwidth within the organization.

For reference, these are not the sorts of trade-offs the section heading means to refer to; those trade-offs (and the ways they complicate the value delivered by Test Engineering) generally follow these earlier ones. For example, perhaps:

- A framework developed by Software Engineering generalists a few years ago isn't extensible or updatable (or it may make failures a challenge to troubleshoot) today because of design decisions made in the past that emphasized rapid- or easy test development at the expense of the former. Maybe the only thing consistent about tests run with these frameworks is that the tests flake; that is, they fail intermittently.
- Automated tests (or libraries responsible for supporting tests) written by production Software Engineers don't exercise the system under test either very efficiently or very expansively. The same with manual tests developed- and executed by resources like Support Engineers and Education Specialists: in each case maybe they cover a majority of use cases (mainly within the realm of immediate concern of the resource that wrote the test), but testing or coverage is not systematized much beyond that. Maybe the implemented testing strategy assumes e2e is the only level worthy of testing at, regardless of whether the system under test is potentially open to testing at a lower level or within a simpler configuration.
- Off-the-shelf test automation tooling (like a tool facilitating UI- or API test automation) that some time ago seemed like a reasonable way to implement or execute testing or coverage without reinventing the wheel now shows its limitations where it comes to workflows and testing scenarios that have surfaced since but which don't fit into the narrow scope of testing that the tool's developers (and marketing apparatus) envisioned would likely excite prospects enough to encourage adoption.

Fortunately the needed work was completed, but today the price for that work lies in the trade-offs that follow the design decisions that made completing the work possible in the past. And the same way any of these trade-offs may complicate the ability of solutions delivered within Test Engineering to justify their own investment, they may also prevent the organization from gathering the business intelligence that originally made testing a strategic investment for the organization to begin with. What's more, the onset of any of issues resulting from these trade-offs may be a challenge to predict.

## Conclusion
The value Test Engineering delivers a software development organization reaches beyond a simple set of checks to cover a specific set of use cases. To anybody who might agree that a simple pass/ fail status is vital to solutions produced by Test Engineering functioning as expected, hopefully it's also agreeable that the value these solutions helps provide is insight into risk related to current functional state, enables departments and teams to work faster and with greater confidence, and reduces the number of unwanted (or unexpected) trade-offs incurred to acquire that information. Within this, Test Engineering conributes to a well-functioning, performant, easy-to-troubleshoot, and extensible feedback loop for the organization's development efforts.

That's if the solutions work; the sorts of issues explored above are what generally occurs when they do not.

As mentioned in the title, though: any of the issues listed above complicate the ability of solutions delivered by Test Engineering to deliver value when allowed negative impact. They may not necessarily invalidate testing efforts outright, but they provide obstruction or complexity to assessment of the relationship between benefit and risk (or, ultimately, benefit and cost), often due to relationships with external factors. Ultimately this obstruction or complexity leads to a point where the relationship between inputs that limits the determinism of the relationship between inputs and outputs. That is to say: if it is easiest to balance the cost of inputs to Test Engineering against the value of outputs when the relationship between the two is simple, the sorts of issues outlined above make any relationship between the two complex and difficult to predict.

As a brief aside, the focus here is not somehow to denigrate or belittle Software Engineers, Support Engineers, or the like. To break the fourth wall briefly, I've served organizations in each of these positions, and beyond this (both when I worked as a Test Engineer and an Automation Engineer), their input (really, most open-ended, constructive feedback from anywhere outside of Test Engineering in general) has generally proven immensely valuable both for solving Test Engineering issues and in general working relationships.

In general pays to work with an expert, if not at least somebody who can begin with the end in mind. The same way this is true with other practices like Software Engineering, End-User Support, and end-user training, it is also true with Test Engineering. Ideally it pays to find somebody who understands the strategic value of testing, the benefits that value can provide to the organization, and how to anticipate (or resolve, if needed) the sorts of issues. Not only can this individual likely help minimize the potential risk for issues like the ones listing here; by doing so hopefully he/ she/ they can also help maximize value while at the same time minimizing overhead.