---

title: "Exploring How to Define a Testing Problem as the Center of a Well-Defined Software Testing Strategy"
publishDate: "2024-08-28T10:41:06-05:00"
coverPhoto: "https://static.trevorwagner.dev/images/alexander-hoggard-Loiv3Mj4qXw-unsplash/1200x675.jpg"
thumbnail: "https://static.trevorwagner.dev/images/alexander-hoggard-Loiv3Mj4qXw-unsplash/300x169.jpg"
draft: false

---

Granted a choice it seems as though, if the most consequential problem to to be solved within Software Test Engineering involved discovering and reporting on potential concerns and risks that challenge the fitness of release candidates to ship, then the most we would need to do is give work product enough of a quick once-over to identify those concerns.

Put somewhat more simply: if the value we expect from Test Engineering (in addition to a usable list of action items proposed to get a release candidate over the line) is feedback either that a solution validates the assumption that the work completed by Software Engineers is ready to ship imminently or that it does not, then it seems like any bug hunt (in whatever structure, if any) should be sufficient to deliver that value.

If this seems like an acceptable definition of the problem Software Test Engineering seeks to solve, then conventional approaches to solving this problem tend to work well until they encounter an issue that produces churn or misses an opportunity for coverage. Once it encounters churn, lack of a strategy focused on solving a testing problem defined clearly in other terms generally (as described above) runs the risk of potential unpredictable shipping delays and need for rework. If testing efforts miss an opportunity for coverage, then it passes on an opportunity to provide a solution fundamental to this definition of the problem. Either outcome produces some amount of waste.

Here are two examples of what this can look like:

- In a common approach that involves **accumulating and maintaining an inventory of test cases** (often executed at the e2e level) **as a list of well-documented user workflows** works well until test execution encounters an issue that prompts reevaluating what should be tested and why. If current coverage is lacking, the team or department will need to compare a mental model of coverage extracted from the list of workflows to a mental model of what should be tested instead. And without an overall strategy for what is important, some teams and departments will opt (without a second thought, sometimes even require) to re-run whole suites of manual- or automated tests.
- Another common approach involves **tasking production Software Engineers with providing coverage of production code they are already familiar with**. The engineers are either required to provide extensive quantified code-level coverage of their solutions or assemble test code to (much like the test case factory described in the last example) cover specific use cases. In addition to leaving the organization open to any risks associated with a test case factory, this approach also leaves software engineers holding the bag for test planning that might not be their bag. To illustrate, [here is a somewhat-recent submission to _The New Stack_, within which the author explains the latter problem from his perspective as a Software Engineer](https://thenewstack.io/unit-tests-are-overrated-rethinking-testing-strategies/).

To get things back on track (remember: shipment is expected imminently), the necessary rework often involves evaluating at least some subset of what needs to get tested (and why), which data or feedback would be useful to satisfy that need, and what means exist to gather that data. At this point, testing finds itself needing to assemble (on the spot) a mental model for what should be tested, why it should be tested, and a plan of attack for how to test it.

If we accept that each example listed above serves as a strategy for testing, then it seems clear that what that opens each to risk for churn is not simply lack of a strategy. Where each strategy leaves its team or organization open to churn, though, is how each attempts to address a problem (related to testing) that remains (at least mostly) unarticulated. Each commits to sets of practices intended to produce visibility without a clear understanding how the need for visibility and available avenues and methods to deliver that visibility present opportunities and constraints for testing clearly targeted at developing feedback and data needed to inform that visibility.

It's these opportunities and constraints, in combination with the role testing plays in ensuring the delivery of value, that define the testing problem. This post will explore how to define a testing problem that can be used to define testing strategy, and within the value the problem provides, attempt to hint at how it lends value to strategy based on it. The more clearly strategists can understand the problem itself as a legitimate engineering problem, the more clearly they should (hopefully) be able to front-load any of the follow-up activity that might normally be needed to resolve churn, by resolving any such issues (identified through a clear definition of the problem itself) proactively.

## Articulate the Need/ Stakes for Testing
If testing serves to produce feedback and data that can be used to inform visibility into the current functional state of work product, what are the contexts within which that visibility would be valuable? Why is testing important to the integrity of the solution? Why is it important to the integrity of business/ organizational operations?

Any feature set finds relevance (and thereby delivers value) by marrying application of technology with a projected strategic advantage for that application. Put simply: we write (nearly any) software because we believe that it will provide some sort of strategic benefit, and to deliver that strategic benefit we apply technology to meet the need for that benefit.

In a business context, business/ organizational interests depend on the ability of the feature set to deliver the expected value (read: consistently). That value may be tied intrinsically to the technical nature of the feature set or it may be enjoyed somewhat more extrinsically (for example, in terms of ease of use or eventual openness to troubleshooting in case of an issue). Failure of a product to deliver value, then, is just that: it's a failure to deliver value that the business or organization expects consumers (and which consumer expect) to be able to enjoy.

The ability to deliver value serves in essence as the business end of a product's value proposition. To articulate the ways in which a value proposition help to define a testing problem, it helps to start with questions like these:

- **What is the Nature of the Solution and the Value it Delivers?** How does the solution apply technology in order to deliver value? At the same time, what is the nature of the value this particular application of technology delivers as a solution? Why is that value of strategic importance to the development organization and/ or the consumers it serves?
- **What is the Nature of any Risk Presented by a Failure to Deliver Value?** If the feature set (provided by applying technology as noted above) were somehow not to behave as expected, what would the consequences of such a failure be? What would the full set of technical consequences be? If the solution were somehow to fail to work as expected, what would the consequences of that failure (or of any related technical consequences) be to the development organization and other relevant stakeholders?

## Articulate the Nature and Value of Output that Can be Gathered from Work Product
Having defined clearly the technical- and business contexts that make the testing problem valuable to solve, it helps to understand which output (if any) can be gathered from the system under test in support of meeting the needs defined by those contexts. Somehow, it should be possible to gather output in support of answering a question about current functionality with a simple yes-or-no answer, and the answers to these question should serve as a piece of the puzzle that, when assembled, provides a somewhat-complete (at least) picture of current functional state as it relates to concerns of value.

To be clear, this is another aspect of the definition of the sort of testing problem that a well-defined testing strategy will seek to solve. The same way the need/ stakes for testing presents a set of parameters that helps define (both in terms of constraints and opportunities) the testing problem, so does the availability of system output and what feedback and/ or data we expect that output to provide.

To articulate the value that the availability of output can lend to defining a testing problem, it helps to start with questions like these:

- **Which Output from the System under Test Is Available to Be Extracted/ Evaluated?** When testing it's important to [identify which output is expected to serve as an answer the clear yes-or-no question or line of inquiry a test is expected to execute in service of](/blog/posts/how-i-write-test-plans-for-new-functionality/). Either a light turns on or it does not. A value gets persisted to a database or it does not. Which output can be extracted from the system under test that serves as a clear signal of the feature set behaving as expected (or not)?
- **How Can Output Be Evaluated as a Means of Producing Feedback and/ or Data?** Any output produced by the system under test delivers a certain amount of value as a signal that the system under test has actually behaved as expected. Which sorts of conclusions does that output serve as evidence in support of? How does an assertion related to this output (in essence, answering the yes-or-no question) fit into the puzzle? Do any conclusions it helps support also help to separate knowns from unknowns?

## Articulate (and Evaluate) Which Options Are Available to Produce Feedback and Data
Within the set of all software, few programs (at least, [as it seems these days](https://www.youtube.com/watch?v=PwZEBE66an0), depending on one's frame of reference) execute completely in isolation; more often than not a solution depends on a matrix of underlying technologies, libraries, runtime environments, hardware, and external integrations in order to support delivering value. In addition, solutions may- or may not provide usable interfaces to be able to interact with them directly (and to collect output for evaluation). Within all of this, available tooling may present access to all or a subset of these interfaces (possibly at cost). At the same time, if the one true constant in software engineering (as with most everything else in life) is the potential for change, a solution today that plans for tomorrow might actually solve more than just one problem.

That last point, by the way, is great for ROI -- even in testing.

Where insights like these ring true, they present additional opportunities and constraints for evaluation of work product. This is yet another aspect of the testing problem that will need to be defined in service of producing a well-defined testing strategy. At the same time this (aspect) presents a set of use cases that will likely be worth targeting in test planning- and execution, it also presents opportunities and constraints to consider when planning testing strategy.

To articulate more clearly the role interaction with the solution itself plays in defining the testing problem, it helps to start with questions like these:

- **What Role Does Runtime Context Play in Solution Operations?** If the solution is expected to operate in a variety of environments, what role might those environments play in the ability of the solution to deliver the expected value? At the same time, which opportunities or constraints might exist that a variety of runtime environments provide to an attempt to test the solution?
- **How Does the System under Test Open Itself to Interaction during Test Runtime?** How does the solution's architecture make it open to gathering output? Is it only open at the e2e level, or is there some lower level (like the integration level, between application layers, or between microservices) that might be useful to gather output from? If the solution makes use of a UI that operates in a runtime separate from the backend, is there an opportunity to test the UI separately from the API? Does the API need to be tested e2e, or can request handling be tested subcutaneously?
- **Which Tooling (or Other Means) are Available to Interact with the System and Collect Output?** How does the availability of (or functionality provided within, or archtecture established by) existing testing tools provide opportunities or constraints to be able to produce and gather output? If a specific Web browser is known for running JavaScript directly on the hardware and a specific Web UI automation tool does not support interaction with that browser, which opportunities or constraints does that present that a corresponding testing strategy should likely account for?
- **Should the Strategy Consider Future Developments?** The more time passes, the greater the degree of likelihood that solutions, the technologies that support them, and (ultimately) the needs that give the solution (as an application of technology) relevance might change over time. Does this escalating degree of likelihood present opportunities for testing? How about constraints?

## Conclusion
The same way [Software Engineers are cautioned to code defensively](https://enterprisecraftsmanship.com/posts/defensive-programming/) (also, in essence, to avoid churn), well-defined test strategy can help individuals, teams, and organizations plan testing defensively, [to minimize the potential for churn in testing at the same time they maximize serviceable and efficient visibility into the functional state of work product](/blog/posts/exploring-the-strategic-business-impact-of-common-test-engineering-issues/). Beyond avoiding churn, if the ideal most efficient&#8482; way to solve a problem involves solving the problem directly, one of the key components for solving a problem both serviceably and efficiently is how clearly one understands the problem.

To be clear, the suggestion here isn't planning for planning's sake. Attempting to solve a problem with an undefined strategy is [the definition of a brute-force approach](https://www.geeksforgeeks.org/brute-force-approach-and-its-pros-and-cons/). If a solution (or its approach to solving a problem, or even the problem itself) is not well defined, the difference of the corresponding strategy from brute force is undetermined unless the solution (and its relationship to the problem) examined for some reason. And because the nature of what should be tested and why can become amorphous if not clearly defined, deliberateness and clarity in defining the problem in addition to (or in concert with) the solution ultimately helps make the definition of each stronger.

As outlined throughout this post, the way to a clear understanding could start with answering questions as simple as these:

1. What value would the feedback and data to be obtained here serve?
2. What would valuable data and feedback look like (in terms of avaialble system output)?
3. Within runtime, which options exist to produce- and gather that feedback?

The approach outlined here is one I have personally found (perennially) works at the organizational/ departmental level, at the team level, and at the individual level to plan an effective approach to comprehensive and efficient testing.

As examples, consider the following:

- If a major change is being made to functionality that is fundamental to the display layer in an N-tier Web app, what is the risk that change poses to the value statement presented by the technical application, and what's a reasonable path of least resistance for the organization to gather data and feedback in response to that risk?
- If multi-statement search functionality presents quintillions of possible combinations for saved searches, what is a path of least resistance to gather the most relevant data and feedback without turning testing into a case-by-case game of Whac-A-Mole?
- What is the value a unit of code (both at a functional level and at a business/ value level within the services it provides to clients of that unit of code) is expected to deliver?

For each, how does it make sense to plan a path forward that capitalizes as much as possible on what we know currently and accounts for potential unknowns (which we can possibly either test for or test in attempt to convert to known state) as we move forward? Meanwhile, if we allow ourselves not to follow through on either, are we comfortable with those unknowns remaining unknown?

By defining the testing problem clearly and making that testing problem the center point for the strategy used to ultimately solve it, hopefully questions like these become easier to solve if they do arise. But at the same time, an organization, team or individual will hopefully not encounter them as often with a good strategy based on a clear understanding of the problem they aim to solve.

_**Quick Footnote:** To be clear, this should hopefully not serve as an exhaustive list. One noteworthy example of another category of feedback that this post intentionally sidesteps because it doesn't fit into the same box as functional validation is the sort of subjective feedback produced through acceptance tesing or unstructured exploratory testing. Another is performance data, which it seems organizations for some reason are asking behavioral test engineers to be prepared to make part of automated quality gates. Yet another involves a business dimension to defining testing problems (for example,_ which subset of users still use Internet Explorer 11?_) related directly to the business ROI for a given set of inquiries._