---

title: "More than a Hot Take: An Assertion Statement Should Serve as the Focal Point for Any Well-Written Automated Test Specification"
publishDate: "2024-06-24T14:14:48-05:00"
coverPhoto: "https://static.trevorwagner.dev/images/balint-mendlik-4-ORHffEh3I-unsplash/1200x794.jpg"
thumbnail: "https://static.trevorwagner.dev/images/balint-mendlik-4-ORHffEh3I-unsplash/300x198.jpg"
draft: false

---

The key things a software development organization expects from automated tests are essentially the same things anybody expects from any testing generally: **serviceability** and **efficiency**. In terms of _serviceability_, automated test specifications are written to execute in support of the the expected inquiries in order to return the intended feedback, to cover the parts of the system under test the organization means to. In addition to things like stability and low overhead in test runtime, _efficiency_ sees automated test specifications that are easy to read, troubleshoot, rewrite, refactor, organize, and even remove as needed. Because every automated test specification has an important job in covering the system under test, each specification has a purpose as it both defines coverage in code and delivers coverage within test runtime.

Where automated tests serve as an important part of overall testing strategy for a software development organization, well-written test specifications help an organization get visibility into the current functional state of work product -- both as specifications execute in runtime and (functioning in essence as documentation) as they are stored within the codebase -- with a minimum of associated overhead.

In nearly every suite of automated tests I can recall having worked within, at least a subset of tests makes use of one of the following approaches to automated test design:

- **Test specifications that do not use assertion statements at all** (for example: the test passes if all operations complete as expected, but at no time is an assertion statement used to compare output extracted from the system under test to expectations).
- **Test specifications that do not establish a clear relationship** between the way output is to be compared to expectations within the test runtime and the expectations or concerns that inform why the test was composed in the first place.
- **Test specifications that use many assertion statements** (whether to evaluate different sets of output or to stop test runtime in cases where state or status do not conform to expectations) of varying relationships (if any) to the specification's general focus for inquiry.

Each of these approaches is problematic for a number of its own reasons. They also share some common general issues related to crafting effective test specifications. Most notably:

- Each of these approaches leaves ambiguous what the intended purpose of a test specification is. Why is this test relevant? This allows test specifications to run either without any relationship between the concerns that inform why testing is performed and the way testing is executed or in a manner where the relationship is ambiguous.
- Each of these approaches challenges the ability of the test specification to define a clear set of constraints for success and clear messaging for feedback in case of failure.
- Each of these approaches limits the ability of the specification to document how it operates and to promote code organization (which can generally be expected to improve code readability) around the key evaluation the specification is responsible for.

Within any organization that depends on tests employing these approach, they (the approach) limit the usefulness of _automated tests as a means to gain clear visibility into current functional state of work product/ the system under test_ and for any engineers who are not experts either in the test code or the system under test to be able to read, troubleshoot, refactor, organize, or rewrite affected specifications.

Much the same way a target in archery provides both focal point and a source of feedback for archers to use to assess the effectiveness of their efforts after round, assertion statements in automated test specifications provide a target for inquiries and a clear source of feedback informing visibility into the current functional state of work product/ the system under test. Disciplined use of assertion statements can help make tests more focused, more effective, and (thereby) their management more efficient for organizations that depend on them.

This post will explore the reasoning that I believe validates this opinion.

## An Assertion Statement Sets a Clear Objective for Inquiry
Every well-written test functions in service of an **inquiry**: in response to some concern (for example, held by stakeholders with respect to how a set of features behaves in runtime), a test is devised to evaluate (in essence, _inquire into_) the current functional state of that feature set. Software development organizations need data in order to be able to make this assessment; testing provides a means to collect this data, within which every test specification serves as one instance data collection.

If it seems agreeable that organizations run automated tests in order to gather feedback (in essence: information) used to assess functional state, then it also seems reasonable that every test specification should (ideally) serve to execute in service of a single inquiry (or as few as possible given the tooling available) into functional state in service of this assessment. It might be worth noting that it is possible in some cases (most readily apparent in but not exclusive to testing at lower levels on the test pyramid) to distinguish functional inquiries from use cases. The more atomic and focused the test is, the better its design will limit the number of inquiries it executes in support of.

Within automated test specifications, an assertion statement defines operational logic that will be used to compare output extracted from the system under test (the _actual result_) to a set of expectations (often referred to as the _expected result_, also provided/ defined as part of the assertion statement) within test runtime. If the values match (or mismatch) as expected, or the actual result otherwise matches the given expectations, the assertion statement generally evaluates to (or returns) `true`, at which point the test passes. In case of failure, the assertion statement generally throws a specific type of error or exception.

Between its role in the composition of a test and its responsibility in delivering business value, then, _an assertion statement functions to define the objective for an automated specification_. It is the designated point where it is confirmed (or not) that the output used as evidence of the feature behaving as expected conforms (or not) to expectations. And it is from this designated point that the specification returns feedback in response to the inquiry the specification executes in support of.

By contrast, a test specification with no assertions has no clear objective. For example, imagine an archer who shoots all day without aiming at a target: despite all of the activity, assessing the success (or failure) of that activity would not be focused, meaningful, or even simple. Along the same lines, the looser the relationship (if any) between the the inquiry the specification executes in service of and the feedback the test produces, the more complex it is to understand what the specification's objective is. Automated tests with multiple assertion statements (especially the more loosely they are related) suffer from this problem and others: imagine, for example, that the same archer was given one arrow to hit multiple targets.

## An Assertion Statement Provides Clear Constraints for Success, and Clear Reporting in Case of Failure
Functionally, at the same time an assertion statement defines operations for the comparison that should occur within an automated test specification, it also defines two other things:

- The aperture for success within the inquiry the specification operates in service of: rather than test runtime simply exiting cleanly, output needs to pass conditions set within the operation at a specific point in test runtime.
- How the test specification will report on a failed comparison (if output does not match expectations specified for the assertion).

If used judiciously, then, an assertion statement should (in these ways) function much like the rings and bullseye on a standard archery target.

As noted above, an assertion statement designates a point within a test specification where _output from the system under test either conforms to expectations or it does not_; as described in the previous section, this is the primary means by which a test is expected to produce feedback in response to the inquiry the specification executes in service of. The narrower the aperture the test specification defines (taking care not to make the test brittle) for the operations that produce this feedback, the clearer the signal can be expected to be both when the specification passes and when it does not. And when the specification does not pass, it's clear that a failure related to a failed assertion relates to the assertion statement (at the point within the specification that the assertion statement is set to compare results), however that statement was defined.

What's more, if it seems agreeable that there is a meaningful difference between a **test failure** (where the comparison defined within an assertion statement fails) and a **test error** (where a test exits with non-passing status for any reason other than a failed assertion statement), then this distinction can help make troubleshooting easier. If a well-written test encounters a test failure, the best place to start looking is within the system under test (the source of the non-matching output). In case of a test error, the search area for a well-written test is wider. [The better the specification can manage integrity in runtime (for example, by use of errors in case state or status depended on by the test becomes non-conformant)](/blog/posts/making-the-most-of-throwing-errors/), the more effectively the specification can generally be expected to enforce the conditions (and thereby the aperture) within which an assertion statement can be expected to fail if (and only if) the system under test produces an unexpected result.

As an alternative, imagine a target in archery with no rings and/ or no bullseye. Or, in another example, imagine where the rings are either non-concentric, non-proportional, non-circular, or generally inconsistent in any of the above. A test specification without an assertion statement assumes no bullseye (and possibly no rings): without a clear indication of conditions for success, it is difficult to judge conformance to expectations other than that the archer hit the target somewhere (or perhaps in a location with an unclear relationship to the main goal: the bullseye). A test specification that does not make the relationship between the inquiry and the assertion statement clear assumes an inconsistent (or unpredictable) ring layout. And the more loosely any of a number of assertion statements connect to either this constraint or the feedback the test is expected to return, the more it resembles a target with many bullseyes.

## An Assertion Statement Establishes a Focal Point around Which to Organize the Specification
If it could be supposed that a test specification works (and reads) like a paragraph, an assertion statement should serve as its topic sentence. In writing composition, **a topic sentence summarizes what the main idea explored within a paragraph is** and provides a clear point of reference the rest of the paragraph can connect to. Much the same way, a well-written test specification supposes that the assertion statement (as the main operation returning data as a result of the query the specification serves in support of) is the "main idea" (as established in a previous section: the _objective_) of the specification, and it will tailor its operations to fit- or complement the assertion statement.

Composing a paragraph (however unaware we may be of it) in support of a single topic or argument rendered within a topic sentence makes a paragraph easier to read: it's easier to follow because it is focused and well-organized. The same way it leaves the composer with a limited number of strategies to thread through additional ideas in a manner that connects them to the main idea, it also leaves the reader with a limited number of paths the paragraph can follow to ultimately resolve. So the better (in conventional writing) a writer can organize thoughts around a topic, an argument, an idea, or a specific train of thought, the better the chances the reader will be able to follow how what's been rendered within the paragraph (also likely with less effort).

This principle is directly transferable to composing readable test specifications, as well: the clearer the clearer an assertions statement is within a test specification, and the clearer the specification can make it how operations defined within the specification relate- or lead to the assertion statement, the easier it will be to follow that thread, as well. And where organizing a paragraph around a topic sentence provides a useful set of constraints that support readability, organizing a test specification around an assertion statement provides the same sorts of constraints that limit opportunities for very similar sorts of readability problems.

Archers do not simply aim for a point on a target: they focus on their goal, they align how they position themselves (and with that, the bow and the arrow) in in support of that focus, and they release. Repeating this over time, they generally develop a sense of awareness and muscle memory that allow them to make minor adjustments with confidence in how those adjustments will affect the outcome once the arrow is released. Because (as noted in a previous section) specifications without assertion statements do not define a goal, they are aimless and lack focus. A specification with multiple unrelated assertion statements supposes many points of focus (with any one possibly worth focus on depending on the situation).

## Conclusion
Well-written automated test specifications should make it as clear as possible how they mean to compare extracted output to expectations, how that comparison will be executed, and how (if there is a mismatch in this comparison) tests can be expected to report on the nature of any mismatch. Ideally, this comparison should serve as both a point of constraint within test runtime (in essence [a moment of truth where the system under test conforms to expectations or it does not](/blog/posts/envisioning-test-specifications-as-a-stage/)) and a point of focus within test composition that informs how the rest or the specification will be organized. Much like targets are used in archery (and, really, any of a number of activities involving shooting), they should serve both as the objective and a source of feedback, by providing a test (literally) of whether the system under test hits- or misses the mark as expected and by making the terms of the evaluation of that effectiveness clear and easy to assess.

Meanwhile, some of the most expensive activities involved in automated testing (beyond executing test runs) frequently involve maintaining them. Troubleshooting failed test runs, refactoring existing specifications and test support code, and organizing- or migrating specifications or suites of tests can all become [the sort operations problem that challenges the effectiveness of testing efforts](/blog/posts/exploring-the-strategic-business-impact-of-common-test-engineering-issues/) if they do not deliver on the following:

- **Clear value/ relevance.** Does the specification add value to delivering feedback that suits the needs of the organization?
- **Clear signal for noise.** In case of failure, is the feedback returned by the specification actionable? In case of success, is the feedback meaningful?
- **Test readability.** Is it easy to get an idea of what the specification is doing without intimate knowledge of either how the specification or the system under test is implemented?
- **Openness to modification/ revision.** How simple does the specification make it either to somehow modify the way test operations have been defined in code?

Assertion statements provide a means for well-written automated test specifications to support all of these things by defining (and limiting focus to) their own target, making the conditions for success clear, and by establishing a focal point to organize the rest of the test with respect to. And the same way they provide a target for the system under test itself, they also provide a target for the authors to focus on, to aim for and seek feedback from, and to focus efforts around. When repeated over time, the discipline outlined here pays off.

As a general rule of thumb, then: **the the fewer assertions** (especially defining comparisons not directly linked to the inquiry the specification has been tasked with executing in support of) **a test specification can make, and the better a test specification can focus on** (thereby organize around) **the limited assertion/s it does make, the better the specification will generally serve as a test**. This can generally be expected to help make testing more **serviceable** (by way of deliberately targeting and aligning around clear conditions for success and failure) and **efficient** (by way of making tests easier to troubleshoot, easier to read, easier to rewrite/ refactor, and easier to organize and potentially remove if needed).

In this, making the assertion statement the focal point of a test specification works a lot like [beginning with the end in mind](https://www.franklincovey.com/the-7-habits/habit-2/): at the same point assertion statements serves as a sort of destination to take aim for in automated test runtime, they also serve as a great place to start when defining that runtime.